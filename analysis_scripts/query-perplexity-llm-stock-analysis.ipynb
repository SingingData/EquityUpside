{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3094c07e",
   "metadata": {},
   "source": [
    "# Stock Analysis Report Generator Using Perplexity\n",
    "\n",
    "<small>\n",
    "\n",
    "#### **Overview**\n",
    "This notebook automatically generates:\n",
    "\n",
    "- Individual stock analysis reports with key financial and growth highlights followed by BuccoCapital 13 point analysis framework\n",
    "- ETF and Mutual Fund analysis with key financial and growth highlights, composition, fees, liquidity, etc in the style of Brian Belsky \n",
    "- Analyst ratings and price targets (when available)\n",
    "\n",
    "This notebook uses Perplexity AI's Sonar Pro model via API. Each report is saved as a formatted Microsoft Word document.6. \n",
    "\n",
    "#### **Features**\n",
    "- Batch Processing: Processes multiple stocks, ETF's and mutual funds from a simple text file list of tickers. \n",
    "- Smart Skipping: Avoids duplicate API calls by checking for existing reports2. 5-year price performance vs. NASDAQ\n",
    "- Markdown Formatting: Converts AI-generated markdown to professional Word formatting1. Stock ticker and generation date\n",
    "- Comprehensive Analysis: Includes price performance charts, financial metrics, and analyst ratingsEach report includes:\n",
    "- Date Stamping: Automatically timestamps each report## Output Format\n",
    "\n",
    "#### **Requirements**\n",
    "- Perplexity API Key- \n",
    "- Input file: Text file with stock tickers (one per line)\n",
    "- Python 3.7+\n",
    "- App like VS Code to run the notebook (run all)\n",
    "- Folder structure locally to get inputs (like equity list) and store outputs (like individual equity reports) Look at getting started .txt file for the right structure and files you need.\n",
    "- Create an environment file (.env file from text) with your API key and specific folder locations of inputs (like equity list) and outputs (for individual reports).  Look at the getting started .txt file for the right way to structure and create your .env file.  \n",
    "\n",
    "#### **Output**\n",
    "- Analysis word document for each equity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae5331",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12a99d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:42:44.364831Z",
     "iopub.status.busy": "2025-11-15T16:42:44.364831Z",
     "iopub.status.idle": "2025-11-15T16:43:06.311528Z",
     "shell.execute_reply": "2025-11-15T16:43:06.311528Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment below pip install lines only if packages are not already installed\n",
    "%pip install -q -r requirements.txt\n",
    "%pip install python-docx\n",
    "\n",
    "# Import necessary packages\n",
    "import sys\n",
    "import requests\n",
    "from docx import Document\n",
    "from docx.shared import Pt, RGBColor\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import os.path\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "import subprocess\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any\n",
    "from IPython.display import Markdown, display, SVG\n",
    "import numpy as np\n",
    "from time import time\n",
    "np.random.seed(10)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e7554",
   "metadata": {},
   "source": [
    "## Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variable informational detail\n",
    "# **MAX_TOKENS**: `2000` - Maximum length of generated response\n",
    "# **INPUT_DIR**: Directory containing the equity list file- **TEMPERATURE**: `0` - Deterministic output for consistent, factual analysis\n",
    "# **OUTPUT_DIR**: Directory where generated reports will be saved- **MODEL**: `sonar-pro` - Perplexity's most advanced model with real-time web access and current financial data\n",
    "# **EQUITY_LIST_FILE**: Text file with stock tickers (one per line)\n",
    "# **Model Settings: Specific model to be invoked, 0 temperature to eliminate creativity and limit the number of tokens for the query.\n",
    "\n",
    "# Load environment variables\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key\n",
    "API_KEY = os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "\n",
    "# Input/Output Configuration\n",
    "INPUT_DIR = os.getenv(\"Input_dir\")\n",
    "OUTPUT_DIR_SEC_FILINGS = os.getenv(\"Output_dir_sec_filings\")\n",
    "OUTPUT_DIR_INDIVIDUAL_STOCK_ANALYSIS = os.getenv(\"Output_dir_individual_equities\")\n",
    "OUTPUT_DIR_PORTFOLIO_ANALYSIS = os.getenv(\"Output_dir_portfolio\")\n",
    "\n",
    "# Prompts and Input Lists\n",
    "EQUITY_LIST_FILE = os.getenv(\"EQUITY_LIST_FILE\")\n",
    "PROMPT_DIR = os.getenv(\"Prompt_dir\")\n",
    "PROMPT_INDIVIDUAL_EQUITY_ANALYSIS = os.getenv(\"PROMPT_INDIVIDUAL_EQUITY_FILE\")\n",
    "PROMPT_PORTFOLIO_ANALYSIS = os.getenv(\"PROMPT_PORTFOLIO_FILE\")\n",
    "PROMPT_RATINGS_CHANGE=os.getenv(\"PROMPT_RATINGS_CHANGE_FILE\")\n",
    "PROMPT_ETF_ANALYSIS_FILE = os.getenv(\"PROMPT_ETF_ANALYSIS_FILE\") \n",
    "\n",
    "# Model Configuration\n",
    "MODEL = \"sonar-pro\" \n",
    "TEMPERATURE = 0\n",
    "MAX_TOKENS = 2000\n",
    "\n",
    "# SEC code header\n",
    "SEC_HEADER =os.getenv(\"User_Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbede98",
   "metadata": {},
   "source": [
    "## Read in Text Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5442a8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:43:06.311528Z",
     "iopub.status.busy": "2025-11-15T16:43:06.311528Z",
     "iopub.status.idle": "2025-11-15T16:43:06.328505Z",
     "shell.execute_reply": "2025-11-15T16:43:06.328505Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read stock list from file\n",
    "with open(EQUITY_LIST_FILE, 'r') as f:\n",
    "    EQUITY_LIST = [line.strip() for line in f if line.strip()]\n",
    "# print small selection of equities from list to verify \n",
    "print(EQUITY_LIST)  # Print equities for verification\n",
    "  \n",
    "# Read individual equity prompt template from file\n",
    "with open(PROMPT_INDIVIDUAL_EQUITY_ANALYSIS, 'r') as f:\n",
    "    PROMPT_TEMPLATE = f.read().strip()\n",
    "#print(PROMPT_TEMPLATE)\n",
    "\n",
    "# Read portfolio prompt template from file\n",
    "with open(PROMPT_PORTFOLIO_ANALYSIS, 'r') as f:\n",
    "    PROMPT_PORTFOLIO_TEMPLATE = f.read().strip()\n",
    "print(\" \")\n",
    "#print(PROMPT_PORTFOLIO_TEMPLATE)\n",
    "\n",
    "# Read ratings change prompt template from file\n",
    "with open(PROMPT_RATINGS_CHANGE, 'r') as f:\n",
    "    PROMPT_RATINGS_CHANGE_TEMPLATE = f.read().strip()\n",
    "print(\" \")\n",
    "#print(PROMPT_RATINGS_CHANGE_TEMPLATE)\n",
    "\n",
    "# Read ETF prompt template from file\n",
    "with open(PROMPT_ETF_ANALYSIS_FILE, 'r') as f:\n",
    "    PROMPT_ETF_ANALYSIS_FILE_TEMPLATE = f.read().strip()\n",
    "print(\" \")\n",
    "#print(PROMPT_ETF_ANALYSIS_FILE_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be69d6",
   "metadata": {},
   "source": [
    "## Define Class to Call Model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72547de2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:43:06.332162Z",
     "iopub.status.busy": "2025-11-15T16:43:06.332162Z",
     "iopub.status.idle": "2025-11-15T16:43:06.338931Z",
     "shell.execute_reply": "2025-11-15T16:43:06.338931Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class PerplexityClient:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=\"https://api.perplexity.ai\"\n",
    "        )\n",
    "\n",
    "    def chat(self, message, model=MODEL):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": message}]\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3003fd4d",
   "metadata": {},
   "source": [
    "## Seperate Individual Equity, ETF's and Other from List of Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tickers(equity_list, client, model=\"sonar-pro\"):\n",
    "    \"\"\"\n",
    "    Classify tickers as Individual Equity, ETF, Mutual Fund, or Other using a single Perplexity API call.\n",
    "    Returns four lists: individual_equities, etfs, mutual_funds, other.\n",
    "    \"\"\"\n",
    "    # Create comma-separated list of tickers\n",
    "    tickers_str = \", \".join(equity_list)\n",
    "    \n",
    "    prompt = f\"\"\"Classify each of the following tickers as one of: Individual Equity, ETF, Mutual Fund, or Other.\n",
    "\n",
    "Tickers: {tickers_str}\n",
    "\n",
    "Return the results in this exact format, one per line:\n",
    "TICKER: CLASSIFICATION\n",
    "\n",
    "Example format:\n",
    "AAPL: Individual Equity\n",
    "SPY: ETF\n",
    "VWICX: Mutual Fund\n",
    "\n",
    "Classify each ticker now:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat(prompt, model=model)\n",
    "        print(\"API Response received. Parsing classifications...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Parse the response\n",
    "        individual_equities = []\n",
    "        etfs = []\n",
    "        mutual_funds = []\n",
    "        other = []\n",
    "        \n",
    "        for line in response.strip().split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line or ':' not in line:\n",
    "                continue\n",
    "            \n",
    "            # Parse \"TICKER: CLASSIFICATION\" format\n",
    "            parts = line.split(':', 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "                \n",
    "            ticker = parts[0].strip().upper()\n",
    "            classification = parts[1].strip().lower()\n",
    "            \n",
    "            # Only process tickers that are in our original list\n",
    "            if ticker not in [t.upper() for t in equity_list]:\n",
    "                continue\n",
    "            \n",
    "            # Find the original case ticker\n",
    "            original_ticker = next((t for t in equity_list if t.upper() == ticker), ticker)\n",
    "            \n",
    "            print(f\"{original_ticker}: {classification}\")\n",
    "            \n",
    "            if \"etf\" in classification:\n",
    "                etfs.append(original_ticker)\n",
    "            elif \"mutual fund\" in classification:\n",
    "                mutual_funds.append(original_ticker)\n",
    "            elif \"individual\" in classification or \"equity\" in classification:\n",
    "                individual_equities.append(original_ticker)\n",
    "            else:\n",
    "                other.append(original_ticker)\n",
    "        \n",
    "        # Check for any tickers that weren't classified\n",
    "        classified = set(t.upper() for t in individual_equities + etfs + mutual_funds + other)\n",
    "        for ticker in equity_list:\n",
    "            if ticker.upper() not in classified:\n",
    "                print(f\"{ticker}: not classified (adding to other)\")\n",
    "                other.append(ticker)\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "        return individual_equities, etfs, mutual_funds, other\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying tickers: {e}\")\n",
    "        # Return all as other if API call fails\n",
    "        return [], [], [], equity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e8dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify tickers and print results\n",
    "if 'individual_equities' not in globals() or not individual_equities:\n",
    "    client = PerplexityClient(api_key=API_KEY)\n",
    "    individual_equities, etfs, mutual_funds, other = classify_tickers(EQUITY_LIST, client)\n",
    "print(\"Individual Equities:\", individual_equities)\n",
    "print(\"ETFs:\", etfs)\n",
    "print(\"Mutual Funds:\", mutual_funds)\n",
    "print(\"Other:\", other)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f13d27b",
   "metadata": {},
   "source": [
    "## Define Functions to Retrieve SEC CIK Codes and Filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873062c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Fetch SEC CIK code for a given ticker symbol\n",
    "def get_cik(ticker):\n",
    "    \"\"\"\n",
    "    Retrieve the CIK (Central Index Key) for a given stock ticker from the SEC database.\n",
    "    Returns the CIK as a zero-padded string if found, otherwise None.\n",
    "    \"\"\"\n",
    "    url = \"https://www.sec.gov/files/company_tickers_exchange.json\"\n",
    "    headers = {\"User-Agent\": SEC_HEADER}\n",
    "    print(\"Fetching CIK for ticker:\", ticker)\n",
    "    \n",
    "    resp = requests.get(url, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"Failed to fetch CIK data. Status code: {resp.status_code}\")\n",
    "        return None\n",
    "    try:\n",
    "        data = resp.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        print(\"Response text:\", resp.text[:200])\n",
    "        return None\n",
    "    # SEC JSON structure: {'fields': [...], 'data': [[...], ...]}\n",
    "    if isinstance(data, dict) and \"fields\" in data and \"data\" in data:\n",
    "        fields = data[\"fields\"]\n",
    "        data_list = data[\"data\"]\n",
    "        # Find the index for 'ticker' and for the CIK field (usually 'cik' or 'cik_str')\n",
    "        ticker_idx = next((i for i, f in enumerate(fields) if f.lower() == \"ticker\"), None)\n",
    "        cik_idx = next((i for i, f in enumerate(fields) if \"cik\" in f.lower()), None)\n",
    "        if ticker_idx is None or cik_idx is None:\n",
    "            print(\"Could not find required fields in SEC data.\")\n",
    "            return None\n",
    "        for entry in data_list:\n",
    "            if entry[ticker_idx].upper() == ticker.upper():\n",
    "                return str(entry[cik_idx]).zfill(10)\n",
    "        print(f\"Ticker {ticker} not found in SEC database.\")\n",
    "        return None\n",
    "    print(\"Unexpected SEC JSON structure.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to Fetch recent SEC filings for a given CIK\n",
    "def get_sec_filings(cik, forms=[\"10-K\", \"10-Q\", \"8-K\"]):\n",
    "    \"\"\"\n",
    "    Retrieve the most recent 10-K, 10-Q, and 8-K filings for a given CIK.\n",
    "    Returns a list of up to 3 filings (one per form type).\n",
    "    \"\"\"\n",
    "    base_url = f\"https://data.sec.gov/submissions/CIK{str(cik).zfill(10)}.json\"\n",
    "    headers = {\"User-Agent\": SEC_HEADER}\n",
    "    try:\n",
    "        resp = requests.get(base_url, headers=headers)\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"Failed to fetch filings for CIK {cik}. Status code: {resp.status_code}\")\n",
    "            return []\n",
    "        data = resp.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching filings for CIK {cik}: {e}\")\n",
    "        return []\n",
    "    filings_dict = {}\n",
    "    recent = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "    forms_list = recent.get(\"form\", [])\n",
    "    accession_list = recent.get(\"accessionNumber\", [])\n",
    "    filing_dates = recent.get(\"filingDate\", [])\n",
    "    primary_docs = recent.get(\"primaryDocument\", [])\n",
    "    for i, form in enumerate(forms_list):\n",
    "        if form in forms and form not in filings_dict:\n",
    "            filing = {\n",
    "                \"form\": form,\n",
    "                \"date\": filing_dates[i] if i < len(filing_dates) else None,\n",
    "                \"accession\": accession_list[i] if i < len(accession_list) else None,\n",
    "                \"url\": f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{accession_list[i].replace('-', '')}/{primary_docs[i]}\" if i < len(accession_list) and i < len(primary_docs) else None,\n",
    "            }\n",
    "            filings_dict[form] = filing\n",
    "            if len(filings_dict) == len(forms):\n",
    "                break\n",
    "    return [filings_dict[form] for form in forms if form in filings_dict]\n",
    "\n",
    "\n",
    "\n",
    "def download_sec_filings(ticker, filings, output_dir):\n",
    "    \"\"\"\n",
    "    Download SEC filings and save them to the specified output directory.\n",
    "    Each file is named as: {ticker}_{form}_{date}.html\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for filing in filings:\n",
    "        url = filing.get(\"url\")\n",
    "        form = filing.get(\"form\")\n",
    "        date = filing.get(\"date\")\n",
    "        if not url or not form or not date:\n",
    "            print(f\"Skipping incomplete filing for {ticker}: {filing}\")\n",
    "            continue\n",
    "        filename = f\"{ticker}_{form}_{date}.html\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        try:\n",
    "            resp = requests.get(url, headers={\"User-Agent\": SEC_HEADER})\n",
    "            if resp.status_code == 200:\n",
    "                with open(filepath, \"wb\") as f:\n",
    "                    f.write(resp.content)\n",
    "                print(f\"Saved: {filepath}\")\n",
    "            else:\n",
    "                print(f\"Failed to download {url} (status {resp.status_code})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae00225",
   "metadata": {},
   "source": [
    "## Save Time by Using Previously Downloaded CIK Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e81d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Summarize and Save CIK Lookup File for Future Use\n",
    "# ============================================================\n",
    "\n",
    "# Load existing CIK lookup file if it exists\n",
    "cik_lookup_file = os.path.join(OUTPUT_DIR_SEC_FILINGS, \"cik_lookup.json\")\n",
    "existing_cik_lookup = {\"individual_equities\": {}, \"etfs\": {}, \"mutual_funds\": {}}\n",
    "\n",
    "if os.path.exists(cik_lookup_file):\n",
    "    with open(cik_lookup_file, 'r') as f:\n",
    "        existing_cik_lookup = json.load(f)\n",
    "    print(f\"✅ Loaded existing CIK lookup file: {cik_lookup_file}\")\n",
    "    print(f\"   - Individual Equities: {len(existing_cik_lookup.get('individual_equities', {}))} CIKs\")\n",
    "    print(f\"   - ETFs: {len(existing_cik_lookup.get('etfs', {}))} CIKs\")\n",
    "    print(f\"   - Mutual Funds: {len(existing_cik_lookup.get('mutual_funds', {}))} CIKs\")\n",
    "else:\n",
    "    print(f\"ℹ️ No existing CIK lookup file found. Will fetch CIKs from SEC.\")\n",
    "\n",
    "# Define variables for each type of investment\n",
    "equity_ciks = dict(existing_cik_lookup.get('individual_equities', {}))\n",
    "mutual_fund_ciks = dict(existing_cik_lookup.get('mutual_funds', {}))\n",
    "etf_ciks = dict(existing_cik_lookup.get('etfs', {}))\n",
    "\n",
    "print(f\"Individual Equities: {len(equity_ciks)} CIKs (filings: 10-K, 10-Q, 8-K)\", flush=True)\n",
    "print(f\"ETFs: {len(etf_ciks)} CIKs (filings: N-CSR, N-CSRS, N-PORT)\", flush=True)\n",
    "print(f\"Mutual Funds: {len(mutual_fund_ciks)} CIKs (filings: N-CSR, N-CSRS, N-PORT)\", flush=True)\n",
    "print(f\"Filings saved to: {OUTPUT_DIR_SEC_FILINGS}\", flush=True)\n",
    "\n",
    "# Create combined CIK lookup dictionary\n",
    "cik_lookup = {\n",
    "    \"individual_equities\": equity_ciks,\n",
    "    \"etfs\": etf_ciks,\n",
    "    \"mutual_funds\": mutual_fund_ciks,\n",
    "    \"metadata\": {\n",
    "        \"generated_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"total_equities\": len(equity_ciks),\n",
    "        \"total_etfs\": len(etf_ciks),\n",
    "        \"total_mutual_funds\": len(mutual_fund_ciks)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save CIK lookup to JSON file\n",
    "cik_lookup_file = os.path.join(OUTPUT_DIR_SEC_FILINGS, \"cik_lookup.json\")\n",
    "with open(cik_lookup_file, 'w') as f:\n",
    "    json.dump(cik_lookup, f, indent=2)\n",
    "print(f\"\\n✅ CIK lookup file saved to: {cik_lookup_file}\", flush=True)\n",
    "\n",
    "# Also create a flat CSV for easy reference\n",
    "cik_flat_list = []\n",
    "for ticker, cik in equity_ciks.items():\n",
    "    cik_flat_list.append({\"ticker\": ticker, \"cik\": cik, \"type\": \"Individual Equity\"})\n",
    "for ticker, cik in etf_ciks.items():\n",
    "    cik_flat_list.append({\"ticker\": ticker, \"cik\": cik, \"type\": \"ETF\"})\n",
    "for ticker, cik in mutual_fund_ciks.items():\n",
    "    cik_flat_list.append({\"ticker\": ticker, \"cik\": cik, \"type\": \"Mutual Fund\"})\n",
    "\n",
    "cik_df = pd.DataFrame(cik_flat_list)\n",
    "cik_csv_file = os.path.join(OUTPUT_DIR_SEC_FILINGS, \"cik_lookup.csv\")\n",
    "cik_df.to_csv(cik_csv_file, index=False)\n",
    "# print(cik_df.head(20))\n",
    "print(f\"✅ CIK lookup CSV saved to: {cik_csv_file}\", flush=True)\n",
    "\n",
    "print(\"\\n✅ SEC processing complete!\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dfb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SEC filings for individual equities, ETFs, and mutual funds\n",
    "import sys\n",
    "from time import sleep\n",
    "\n",
    "# SEC EDGAR Rate Limiting: Max 10 requests per second\n",
    "# We use 0.15 second delay (~6.6 requests/sec) to stay safely under the limit\n",
    "SEC_REQUEST_DELAY = 0.15  # seconds between requests\n",
    "SEC_MAX_RETRIES = 3       # number of retry attempts on failure\n",
    "\n",
    "def sec_request_with_retry(url, headers, max_retries=SEC_MAX_RETRIES, delay=SEC_REQUEST_DELAY):\n",
    "    \"\"\"\n",
    "    Make a request to SEC EDGAR with rate limiting and retry logic.\n",
    "    Handles ConnectionResetError and other transient failures.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            sleep(delay)  # Rate limiting delay\n",
    "            resp = requests.get(url, headers=headers, timeout=30)\n",
    "            return resp\n",
    "        except (requests.exceptions.ConnectionError, \n",
    "                requests.exceptions.Timeout,\n",
    "                ConnectionResetError) as e:\n",
    "            wait_time = (attempt + 1) * 2  # Exponential backoff: 2, 4, 6 seconds\n",
    "            print(f\"    ⚠️ Connection error (attempt {attempt + 1}/{max_retries}): {type(e).__name__}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"    Waiting {wait_time} seconds before retry...\", flush=True)\n",
    "                sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"    ❌ Failed after {max_retries} attempts\", flush=True)\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "# Helper function to get CIK for stocks/ETFs (exchange-listed securities)\n",
    "def fetch_cik(ticker, headers):\n",
    "    \"\"\"Fetch CIK for stocks and ETFs from company_tickers_exchange.json\"\"\"\n",
    "    url = \"https://www.sec.gov/files/company_tickers_exchange.json\"\n",
    "    resp = sec_request_with_retry(url, headers)\n",
    "    if resp and resp.status_code == 200:\n",
    "        data = resp.json()\n",
    "        fields = data.get(\"fields\", [])\n",
    "        data_list = data.get(\"data\", [])\n",
    "        ticker_idx = next((i for i, f in enumerate(fields) if f.lower() == \"ticker\"), None)\n",
    "        cik_idx = next((i for i, f in enumerate(fields) if \"cik\" in f.lower()), None)\n",
    "        if ticker_idx is not None and cik_idx is not None:\n",
    "            for entry in data_list:\n",
    "                if entry[ticker_idx].upper() == ticker.upper():\n",
    "                    return str(entry[cik_idx]).zfill(10)\n",
    "    return None\n",
    "\n",
    "# Helper function to get CIK for mutual funds (different SEC file)\n",
    "def fetch_mutual_fund_cik(ticker, headers):\n",
    "    \"\"\"Fetch CIK for mutual funds from company_tickers_mf.json\"\"\"\n",
    "    url = \"https://www.sec.gov/files/company_tickers_mf.json\"\n",
    "    resp = sec_request_with_retry(url, headers)\n",
    "    if resp and resp.status_code == 200:\n",
    "        data = resp.json()\n",
    "        # Mutual fund JSON structure: {\"fields\": [...], \"data\": [[...], ...]}\n",
    "        # fields = ['cik', 'seriesId', 'classId', 'symbol']\n",
    "        fields = data.get(\"fields\", [])\n",
    "        data_list = data.get(\"data\", [])\n",
    "        \n",
    "        # Find indices for symbol and cik\n",
    "        symbol_idx = fields.index(\"symbol\") if \"symbol\" in fields else None\n",
    "        cik_idx = fields.index(\"cik\") if \"cik\" in fields else None\n",
    "        \n",
    "        if symbol_idx is not None and cik_idx is not None:\n",
    "            for row in data_list:\n",
    "                if row[symbol_idx].upper() == ticker.upper():\n",
    "                    return str(row[cik_idx]).zfill(10)\n",
    "    return None\n",
    "\n",
    "headers = {\"User-Agent\": SEC_HEADER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a024ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. Process INDIVIDUAL EQUITIES - Get CIK and download 10-K, 10-Q, 8-K\n",
    "# ============================================================\n",
    "print(\"=\" * 60, flush=True)\n",
    "print(\"INDIVIDUAL EQUITIES - Fetching CIK and SEC filings (10-K, 10-Q, 8-K)\", flush=True)\n",
    "print(\"=\" * 60, flush=True)\n",
    "\n",
    "# Start with existing CIKs from lookup file\n",
    "equity_ciks = dict(existing_cik_lookup.get('individual_equities', {}))\n",
    "fetched_count = 0\n",
    "cached_count = 0\n",
    "\n",
    "for ticker in individual_equities:\n",
    "    print(f\"--- Processing {ticker} ---\", flush=True)\n",
    "    \n",
    "    # Check if CIK already exists in lookup\n",
    "    if ticker in equity_ciks:\n",
    "        cik = equity_ciks[ticker]\n",
    "        print(f\"  CIK: {cik} (cached)\", flush=True)\n",
    "        cached_count += 1\n",
    "    else:\n",
    "        # Fetch CIK from SEC\n",
    "        cik = fetch_cik(ticker, headers)\n",
    "        if cik:\n",
    "            equity_ciks[ticker] = cik\n",
    "            print(f\"  CIK: {cik} (fetched)\", flush=True)\n",
    "            fetched_count += 1\n",
    "        else:\n",
    "            print(f\"  Could not find CIK\", flush=True)\n",
    "            continue\n",
    "    \n",
    "    # Fetch filings for individual equities\n",
    "    base_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "    resp = sec_request_with_retry(base_url, headers)\n",
    "    filings = []\n",
    "    if resp and resp.status_code == 200:\n",
    "        data = resp.json()\n",
    "        recent = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "        forms_list = recent.get(\"form\", [])\n",
    "        accession_list = recent.get(\"accessionNumber\", [])\n",
    "        filing_dates = recent.get(\"filingDate\", [])\n",
    "        primary_docs = recent.get(\"primaryDocument\", [])\n",
    "        seen_forms = set()\n",
    "        for i, form in enumerate(forms_list):\n",
    "            if form in [\"10-K\", \"10-Q\", \"8-K\"] and form not in seen_forms:\n",
    "                filings.append({\n",
    "                    \"form\": form,\n",
    "                    \"date\": filing_dates[i],\n",
    "                    \"url\": f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{accession_list[i].replace('-', '')}/{primary_docs[i]}\"\n",
    "                })\n",
    "                seen_forms.add(form)\n",
    "                if len(seen_forms) == 3:\n",
    "                    break\n",
    "    \n",
    "    print(f\"  Found {len(filings)} filings\", flush=True)\n",
    "    \n",
    "    # Download filings\n",
    "    os.makedirs(OUTPUT_DIR_SEC_FILINGS, exist_ok=True)\n",
    "    for filing in filings:\n",
    "        filename = f\"{ticker}_{filing['form']}_{filing['date']}.html\"\n",
    "        filepath = os.path.join(OUTPUT_DIR_SEC_FILINGS, filename)\n",
    "        resp = sec_request_with_retry(filing['url'], headers)\n",
    "        if resp and resp.status_code == 200:\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                f.write(resp.content)\n",
    "            print(f\"    Saved: {filename}\", flush=True)\n",
    "        else:\n",
    "            print(f\"    ⚠️ Failed to download: {filename}\", flush=True)\n",
    "\n",
    "print(f\"\\n✅ Individual equities complete: {len(equity_ciks)} CIKs ({cached_count} cached, {fetched_count} fetched)\\n\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. Process ETFs - Get CIK and download Fund SEC Filings\n",
    "# Note: Some ETFs (especially open-end fund ETFs) are in the mutual fund file\n",
    "# ============================================================\n",
    "print(\"=\" * 60, flush=True)\n",
    "print(\"ETFs - Fetching CIK and SEC filings\", flush=True)\n",
    "print(\"=\" * 60, flush=True)\n",
    "\n",
    "# Define all fund-related SEC forms to fetch:\n",
    "# - Form N-1A: Statutory Prospectus and SAI (initial registration)\n",
    "# - 485BPOS/485APOS: Post-effective amendments to Form N-1A\n",
    "# - N-CSR: Annual shareholder report (certified)\n",
    "# - N-CSRS: Semi-annual shareholder report\n",
    "# - N-PORT/N-PORT-P: Portfolio holdings report\n",
    "# - N-CEN: Annual report for registered investment companies\n",
    "FUND_FORMS = {\n",
    "    # Prospectus & Registration Forms\n",
    "    \"N-1A\": \"N-1A_Prospectus_SAI\",           # Initial Form N-1A (statutory prospectus + SAI)\n",
    "    \"N-1A/A\": \"N-1A_Amendment\",               # Amendment to Form N-1A\n",
    "    \"485BPOS\": \"N-1A_Post_Effective_Amend\",   # Post-effective amendment to Form N-1A\n",
    "    \"485APOS\": \"N-1A_Pre_Effective_Amend\",    # Pre-effective amendment to Form N-1A\n",
    "    # Shareholder Reports\n",
    "    \"N-CSR\": \"Annual_Shareholder_Report\",     # Annual shareholder report\n",
    "    \"N-CSRS\": \"Semiannual_Shareholder_Report\", # Semi-annual shareholder report\n",
    "    # Portfolio & Annual Reports\n",
    "    \"N-PORT\": \"Portfolio_Holdings\",           # Monthly portfolio holdings\n",
    "    \"N-PORT-P\": \"Portfolio_Holdings\",         # Portfolio holdings (partial)\n",
    "    \"N-CEN\": \"Annual_Report_N-CEN\",           # Annual report for investment companies\n",
    "}\n",
    "\n",
    "# Start with existing CIKs from lookup file\n",
    "etf_ciks = dict(existing_cik_lookup.get('etfs', {}))\n",
    "fetched_count = 0\n",
    "cached_count = 0\n",
    "\n",
    "for ticker in etfs:\n",
    "    print(f\"--- Processing {ticker} ---\", flush=True)\n",
    "    \n",
    "    # Check if CIK already exists in lookup\n",
    "    if ticker in etf_ciks:\n",
    "        cik = etf_ciks[ticker]\n",
    "        print(f\"  CIK: {cik} (cached)\", flush=True)\n",
    "        cached_count += 1\n",
    "    else:\n",
    "        # First try exchange file (standard ETFs)\n",
    "        cik = fetch_cik(ticker, headers)\n",
    "        if not cik:\n",
    "            # Fallback to mutual fund file (open-end fund ETFs like VNM)\n",
    "            cik = fetch_mutual_fund_cik(ticker, headers)\n",
    "        if cik:\n",
    "            etf_ciks[ticker] = cik\n",
    "            print(f\"  CIK: {cik} (fetched)\", flush=True)\n",
    "            fetched_count += 1\n",
    "        else:\n",
    "            print(f\"  Could not find CIK in exchange or mutual fund files\", flush=True)\n",
    "            continue\n",
    "    \n",
    "    # Fetch filings for ETFs - get latest of each form type\n",
    "    base_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "    resp = sec_request_with_retry(base_url, headers)\n",
    "    filings = []\n",
    "    if resp and resp.status_code == 200:\n",
    "        data = resp.json()\n",
    "        recent = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "        forms_list = recent.get(\"form\", [])\n",
    "        accession_list = recent.get(\"accessionNumber\", [])\n",
    "        filing_dates = recent.get(\"filingDate\", [])\n",
    "        primary_docs = recent.get(\"primaryDocument\", [])\n",
    "        \n",
    "        # Track which form types we've found (get latest of each)\n",
    "        found_forms = set()\n",
    "        \n",
    "        for i, form in enumerate(forms_list):\n",
    "            # Check if this form is in our target list and we haven't found it yet\n",
    "            if form in FUND_FORMS and form not in found_forms:\n",
    "                doc_type = FUND_FORMS[form]\n",
    "                filings.append({\n",
    "                    \"form\": form,\n",
    "                    \"type\": doc_type,\n",
    "                    \"date\": filing_dates[i],\n",
    "                    \"url\": f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{accession_list[i].replace('-', '')}/{primary_docs[i]}\"\n",
    "                })\n",
    "                found_forms.add(form)\n",
    "                print(f\"  Found {form}: {doc_type} ({filing_dates[i]})\", flush=True)\n",
    "            \n",
    "            # Stop once we have all form types\n",
    "            if len(found_forms) == len(FUND_FORMS):\n",
    "                break\n",
    "    \n",
    "    print(f\"  Total filings to download: {len(filings)}\", flush=True)\n",
    "    \n",
    "    # Download filings\n",
    "    if filings:\n",
    "        os.makedirs(OUTPUT_DIR_SEC_FILINGS, exist_ok=True)\n",
    "        for filing in filings:\n",
    "            # Replace slashes in form names for valid filenames\n",
    "            form_name = filing['form'].replace('/', '-')\n",
    "            doc_type = filing['type'].replace(' ', '_')\n",
    "            filename = f\"{ticker}_{doc_type}_{form_name}_{filing['date']}.html\"\n",
    "            filepath = os.path.join(OUTPUT_DIR_SEC_FILINGS, filename)\n",
    "            resp = sec_request_with_retry(filing['url'], headers)\n",
    "            if resp and resp.status_code == 200:\n",
    "                with open(filepath, \"wb\") as f:\n",
    "                    f.write(resp.content)\n",
    "                print(f\"    Saved: {filename}\", flush=True)\n",
    "            else:\n",
    "                print(f\"    ⚠️ Failed to download: {filename}\", flush=True)\n",
    "\n",
    "print(f\"\\n✅ ETFs complete: {len(etf_ciks)} CIKs ({cached_count} cached, {fetched_count} fetched)\\n\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa36059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download filings\n",
    "# ============================================================\n",
    "# 3. Process MUTUAL FUNDS - Get CIK and download SEC Filings\n",
    "# ============================================================\n",
    "print(\"=\" * 60, flush=True)\n",
    "print(\"MUTUAL FUNDS - Fetching CIK and SEC filings\", flush=True)\n",
    "print(\"=\" * 60, flush=True)\n",
    "\n",
    "# Start with existing CIKs from lookup file\n",
    "mutual_fund_ciks = dict(existing_cik_lookup.get('mutual_funds', {}))\n",
    "fetched_count = 0\n",
    "cached_count = 0\n",
    "\n",
    "for ticker in mutual_funds:\n",
    "    print(f\"--- Processing {ticker} ---\", flush=True)\n",
    "    \n",
    "    # Check if CIK already exists in lookup\n",
    "    if ticker in mutual_fund_ciks:\n",
    "        cik = mutual_fund_ciks[ticker]\n",
    "        print(f\"  CIK: {cik} (cached)\", flush=True)\n",
    "        cached_count += 1\n",
    "    else:\n",
    "        # Use mutual fund specific function\n",
    "        cik = fetch_mutual_fund_cik(ticker, headers)\n",
    "        if cik:\n",
    "            mutual_fund_ciks[ticker] = cik\n",
    "            print(f\"  CIK: {cik} (fetched)\", flush=True)\n",
    "            fetched_count += 1\n",
    "        else:\n",
    "            print(f\"  Could not find CIK\", flush=True)\n",
    "            continue\n",
    "    \n",
    "    # Fetch filings for mutual funds - get latest of each form type\n",
    "    base_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "    resp = sec_request_with_retry(base_url, headers)\n",
    "    filings = []\n",
    "    if resp and resp.status_code == 200:\n",
    "        data = resp.json()\n",
    "        recent = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "        forms_list = recent.get(\"form\", [])\n",
    "        accession_list = recent.get(\"accessionNumber\", [])\n",
    "        filing_dates = recent.get(\"filingDate\", [])\n",
    "        primary_docs = recent.get(\"primaryDocument\", [])\n",
    "        \n",
    "        # Track which form types we've found (get latest of each)\n",
    "        found_forms = set()\n",
    "        \n",
    "        for i, form in enumerate(forms_list):\n",
    "            # Check if this form is in our target list and we haven't found it yet\n",
    "            if form in FUND_FORMS and form not in found_forms:\n",
    "                doc_type = FUND_FORMS[form]\n",
    "                filings.append({\n",
    "                    \"form\": form,\n",
    "                    \"type\": doc_type,\n",
    "                    \"date\": filing_dates[i],\n",
    "                    \"url\": f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{accession_list[i].replace('-', '')}/{primary_docs[i]}\"\n",
    "                })\n",
    "                found_forms.add(form)\n",
    "                print(f\"  Found {form}: {doc_type} ({filing_dates[i]})\", flush=True)\n",
    "            \n",
    "            # Stop once we have all form types\n",
    "            if len(found_forms) == len(FUND_FORMS):\n",
    "                break\n",
    "    \n",
    "    print(f\"  Total filings to download: {len(filings)}\", flush=True)\n",
    "    \n",
    "    \n",
    "    if filings:\n",
    "        os.makedirs(OUTPUT_DIR_SEC_FILINGS, exist_ok=True)\n",
    "        for filing in filings:\n",
    "            # Replace slashes in form names for valid filenames\n",
    "            form_name = filing['form'].replace('/', '-')\n",
    "            doc_type = filing['type'].replace(' ', '_')\n",
    "            filename = f\"{ticker}_{doc_type}_{form_name}_{filing['date']}.html\"\n",
    "            filepath = os.path.join(OUTPUT_DIR_SEC_FILINGS, filename)\n",
    "            resp = sec_request_with_retry(filing['url'], headers)\n",
    "            if resp and resp.status_code == 200:\n",
    "                with open(filepath, \"wb\") as f:\n",
    "                    f.write(resp.content)\n",
    "                print(f\"    Saved: {filename}\", flush=True)\n",
    "            else:\n",
    "                print(f\"    ⚠️ Failed to download: {filename}\", flush=True)\n",
    "\n",
    "print(f\"\\n✅ Mutual Funds complete: {len(mutual_fund_ciks)} CIKs ({cached_count} cached, {fetched_count} fetched)\\n\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf5677",
   "metadata": {},
   "source": [
    "## Define Classes to Format the Output in a Word Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860289cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:43:06.339940Z",
     "iopub.status.busy": "2025-11-15T16:43:06.339940Z",
     "iopub.status.idle": "2025-11-15T16:43:06.359281Z",
     "shell.execute_reply": "2025-11-15T16:43:06.359281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert markdown to Word document content\n",
    "def add_markdown_to_word(doc, markdown_text):\n",
    "    \"\"\"Convert markdown text to formatted Word document content.\n",
    "    Uses narrative paragraphs for most content.\n",
    "    Bullet points only for true lists (multiple consecutive short items under a header).\n",
    "    Bold is only used for titles, section headers, and table headers.\n",
    "    \"\"\"\n",
    "    lines = markdown_text.split('\\n')\n",
    "    i = 0\n",
    "    in_list_context = False  # Track if we're in a list context (after header or multiple bullets)\n",
    "    consecutive_bullets = 0   # Count consecutive bullet items\n",
    "    \n",
    "    def is_list_item(text):\n",
    "        \"\"\"Check if text looks like a list item (short, data-like content).\"\"\"\n",
    "        if len(text) < 100:\n",
    "            return True\n",
    "        if re.search(r':\\s*[\\d$%]|[\\d.]+%|\\$[\\d,.]+|\\d+\\s*(million|billion|M|B|K)', text):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def count_upcoming_bullets(lines, start_idx):\n",
    "        \"\"\"Count how many consecutive bullet lines follow.\"\"\"\n",
    "        count = 0\n",
    "        for j in range(start_idx, len(lines)):\n",
    "            line = lines[j].strip()\n",
    "            if line.startswith(('- ', '* ', '• ')):\n",
    "                count += 1\n",
    "            elif line == '':\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        return count\n",
    "    \n",
    "    def extract_styled_header(line):\n",
    "        \"\"\"Extract text from styled headers like **<span style=\"...\">1. Title</span>**\n",
    "        Returns (clean_text, is_styled_header)\n",
    "        \"\"\"\n",
    "        # Pattern for **<span style=\"...\">text</span>** format\n",
    "        span_pattern = r'\\*\\*<span[^>]*>(.+?)</span>\\*\\*'\n",
    "        match = re.search(span_pattern, line)\n",
    "        if match:\n",
    "            return match.group(1).strip(), True\n",
    "        \n",
    "        # Also handle <span> without outer **\n",
    "        span_pattern2 = r'<span[^>]*>(.+?)</span>'\n",
    "        match2 = re.search(span_pattern2, line)\n",
    "        if match2:\n",
    "            return match2.group(1).strip(), True\n",
    "        \n",
    "        return None, False\n",
    "    \n",
    "    def clean_html_tags(text):\n",
    "        \"\"\"Remove any HTML tags from text.\"\"\"\n",
    "        return re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        \n",
    "        # Skip empty lines\n",
    "        if not line.strip():\n",
    "            i += 1\n",
    "            in_list_context = False\n",
    "            consecutive_bullets = 0\n",
    "            continue\n",
    "        \n",
    "        # Check for styled numbered headers (e.g., **<span style=\"...\">1. Title</span>**)\n",
    "        styled_text, is_styled = extract_styled_header(line)\n",
    "        if is_styled:\n",
    "            p = doc.add_paragraph()\n",
    "            run = p.add_run(styled_text)\n",
    "            run.bold = True\n",
    "            run.font.size = Pt(14)  # Larger than normal text\n",
    "            run.font.color.rgb = RGBColor(0, 0, 139)  # Dark blue color\n",
    "            in_list_context = True\n",
    "            consecutive_bullets = 0\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # Headers (# ## ###) - These get bold via heading styles\n",
    "        if line.startswith('#'):\n",
    "            level = len(line) - len(line.lstrip('#'))\n",
    "            text = line.lstrip('#').strip()\n",
    "            text = clean_html_tags(text)  # Remove any HTML tags\n",
    "            doc.add_heading(text, level=min(level, 9))\n",
    "            in_list_context = True\n",
    "            consecutive_bullets = 0\n",
    "        \n",
    "        # Tables (|...) - Keep table formatting as-is\n",
    "        elif '|' in line and i + 1 < len(lines) and '|' in lines[i + 1]:\n",
    "            table_lines = [line]\n",
    "            i += 1\n",
    "            if '---' in lines[i] or ':-:' in lines[i]:\n",
    "                i += 1\n",
    "            while i < len(lines) and '|' in lines[i]:\n",
    "                table_lines.append(lines[i])\n",
    "                i += 1\n",
    "            \n",
    "            headers = [cell.strip() for cell in table_lines[0].split('|') if cell.strip()]\n",
    "            num_cols = len(headers)\n",
    "            num_rows = len(table_lines)\n",
    "            \n",
    "            table = doc.add_table(rows=num_rows, cols=num_cols)\n",
    "            table.style = 'Light Grid Accent 1'\n",
    "            \n",
    "            for j, header in enumerate(headers):\n",
    "                cell = table.rows[0].cells[j]\n",
    "                header_text = clean_html_tags(header.replace('**', '').replace('__', ''))\n",
    "                cell.text = header_text\n",
    "                cell.paragraphs[0].runs[0].bold = True\n",
    "            \n",
    "            for row_idx in range(1, len(table_lines)):\n",
    "                cells = [cell.strip() for cell in table_lines[row_idx].split('|') if cell.strip()]\n",
    "                for col_idx, cell_text in enumerate(cells):\n",
    "                    if col_idx < num_cols:\n",
    "                        clean_text = clean_html_tags(cell_text.replace('**', '').replace('__', ''))\n",
    "                        table.rows[row_idx].cells[col_idx].text = clean_text\n",
    "            \n",
    "            doc.add_paragraph()\n",
    "            in_list_context = False\n",
    "            consecutive_bullets = 0\n",
    "            continue\n",
    "        \n",
    "        # Bullet points (- or *) - Only use bullets for true lists\n",
    "        elif line.strip().startswith(('- ', '* ', '• ')):\n",
    "            text = line.strip()[2:].strip()\n",
    "            text = clean_html_tags(text)  # Remove any HTML tags\n",
    "            \n",
    "            if consecutive_bullets == 0:\n",
    "                upcoming = count_upcoming_bullets(lines, i)\n",
    "            else:\n",
    "                upcoming = consecutive_bullets\n",
    "            \n",
    "            use_bullet = (upcoming >= 3) or (in_list_context and is_list_item(text) and upcoming >= 2)\n",
    "            \n",
    "            if use_bullet:\n",
    "                p = doc.add_paragraph(style='List Bullet')\n",
    "                add_formatted_text(p, text, allow_bold=False)\n",
    "                consecutive_bullets = upcoming\n",
    "            else:\n",
    "                p = doc.add_paragraph()\n",
    "                add_formatted_text(p, text, allow_bold=False)\n",
    "                consecutive_bullets = 0\n",
    "            \n",
    "            in_list_context = False\n",
    "        \n",
    "        # Numbered lists (1. 2. etc) - Check if it has HTML styling first\n",
    "        elif re.match(r'^\\d+\\.\\s', line.strip()):\n",
    "            # Check if the line contains HTML span styling\n",
    "            if '<span' in line:\n",
    "                styled_text, is_styled = extract_styled_header(line)\n",
    "                if is_styled:\n",
    "                    p = doc.add_paragraph()\n",
    "                    run = p.add_run(styled_text)\n",
    "                    run.bold = True\n",
    "                    run.font.size = Pt(14)\n",
    "                    run.font.color.rgb = RGBColor(0, 0, 139)\n",
    "                    in_list_context = True\n",
    "                    consecutive_bullets = 0\n",
    "                    i += 1\n",
    "                    continue\n",
    "            \n",
    "            # Regular numbered list without HTML styling\n",
    "            text = re.sub(r'^\\d+\\.\\s', '', line.strip())\n",
    "            text = clean_html_tags(text)\n",
    "            p = doc.add_paragraph(style='List Number')\n",
    "            add_formatted_text(p, text, allow_bold=True)\n",
    "            for run in p.runs:\n",
    "                run.font.size = Pt(14)\n",
    "                run.bold = True\n",
    "            in_list_context = True\n",
    "            consecutive_bullets = 0\n",
    "        \n",
    "        # Bold text lines that appear to be section headers\n",
    "        elif (line.strip().startswith('**') and line.strip().endswith('**') and \n",
    "              len(line.strip()) < 100 and '\\n' not in line):\n",
    "            # Check for HTML inside\n",
    "            if '<span' in line:\n",
    "                styled_text, is_styled = extract_styled_header(line)\n",
    "                if is_styled:\n",
    "                    p = doc.add_paragraph()\n",
    "                    run = p.add_run(styled_text)\n",
    "                    run.bold = True\n",
    "                    run.font.size = Pt(14)\n",
    "                    run.font.color.rgb = RGBColor(0, 0, 139)\n",
    "                    in_list_context = True\n",
    "                    consecutive_bullets = 0\n",
    "                    i += 1\n",
    "                    continue\n",
    "            \n",
    "            p = doc.add_paragraph()\n",
    "            clean_line = clean_html_tags(line)\n",
    "            add_formatted_text(p, clean_line, allow_bold=True)\n",
    "            in_list_context = True\n",
    "            consecutive_bullets = 0\n",
    "        \n",
    "        # Regular paragraph - no bold, no bullets\n",
    "        else:\n",
    "            p = doc.add_paragraph()\n",
    "            clean_line = clean_html_tags(line)\n",
    "            add_formatted_text(p, clean_line, allow_bold=False)\n",
    "            in_list_context = False\n",
    "            consecutive_bullets = 0\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "def add_formatted_text(paragraph, text, allow_bold=False):\n",
    "    \"\"\"Add text with optional bold/italic markdown formatting to a paragraph.\n",
    "    \n",
    "    Args:\n",
    "        paragraph: The Word document paragraph to add text to\n",
    "        text: The text content (may contain markdown formatting)\n",
    "        allow_bold: If True, apply bold to **text**. If False, strip bold markers.\n",
    "    \"\"\"\n",
    "    # First remove any HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    parts = re.split(r'(\\*\\*.*?\\*\\*|__.*?__|`.*?`)', text)\n",
    "    \n",
    "    for part in parts:\n",
    "        if not part:\n",
    "            continue\n",
    "        \n",
    "        if part.startswith('**') and part.endswith('**'):\n",
    "            run = paragraph.add_run(part[2:-2])\n",
    "            if allow_bold:\n",
    "                run.bold = True\n",
    "        elif part.startswith('__') and part.endswith('__'):\n",
    "            run = paragraph.add_run(part[2:-2])\n",
    "            if allow_bold:\n",
    "                run.bold = True\n",
    "        elif part.startswith('`') and part.endswith('`'):\n",
    "            run = paragraph.add_run(part[1:-1])\n",
    "            run.font.name = 'Courier New'\n",
    "        else:\n",
    "            paragraph.add_run(part)\n",
    "\n",
    "\n",
    "def sort_markdown_table_by_industry(markdown_text):\n",
    "    lines = markdown_text.split('\\n')\n",
    "    table_start = None\n",
    "    table_end = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if '|' in line and table_start is None:\n",
    "            table_start = i\n",
    "        elif table_start is not None and ('|' not in line or not line.strip()):\n",
    "            table_end = i\n",
    "            break\n",
    "    if table_start is None or table_end is None:\n",
    "        return markdown_text\n",
    "\n",
    "    header = lines[table_start]\n",
    "    separator = lines[table_start + 1]\n",
    "    rows = lines[table_start + 2:table_end]\n",
    "    columns = [col.strip().lower() for col in header.split('|')]\n",
    "    try:\n",
    "        industry_idx = columns.index('industry')\n",
    "    except ValueError:\n",
    "        try:\n",
    "            industry_idx = columns.index('category')\n",
    "        except ValueError:\n",
    "            return markdown_text\n",
    "\n",
    "    def get_industry(row):\n",
    "        cells = [cell.strip() for cell in row.split('|')]\n",
    "        return cells[industry_idx] if industry_idx < len(cells) else ''\n",
    "    rows_sorted = sorted(rows, key=get_industry)\n",
    "\n",
    "    sorted_table = [header, separator] + rows_sorted\n",
    "    lines = lines[:table_start] + sorted_table + lines[table_end:]\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ace9bb6",
   "metadata": {},
   "source": [
    "## Generate Lookup List of SEC Documents in SEC Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3480a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of SEC documents in OUTPUT_DIR_SEC_FILINGS and save to file\n",
    "sec_files = []\n",
    "if os.path.exists(OUTPUT_DIR_SEC_FILINGS):\n",
    "    sec_files = sorted(os.listdir(OUTPUT_DIR_SEC_FILINGS))\n",
    "    print(f\"SEC Filings Directory: {OUTPUT_DIR_SEC_FILINGS}\")\n",
    "    print(f\"Total files: {len(sec_files)}\\n\")\n",
    "    print(\"-\" * 60)\n",
    "    #for f in sec_files:\n",
    "        #print(f)\n",
    "    #print(\"-\" * 60)\n",
    "    \n",
    "    # Save the list to a text file in the SEC filings directory\n",
    "    sec_files_list_path = os.path.join(OUTPUT_DIR_SEC_FILINGS, \"sec_filings_list.txt\")\n",
    "    with open(sec_files_list_path, 'w') as f:\n",
    "        f.write(f\"SEC Filings List - Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Directory: {OUTPUT_DIR_SEC_FILINGS}\\n\")\n",
    "        f.write(f\"Total files: {len(sec_files)}\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        for sec_file in sec_files:\n",
    "            f.write(f\"{sec_file}\\n\")\n",
    "    print(f\"\\n✅ SEC filings list saved to: {sec_files_list_path}\")\n",
    "else:\n",
    "    print(f\"Directory not found: {OUTPUT_DIR_SEC_FILINGS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b86f8",
   "metadata": {},
   "source": [
    "## Build a dictionary of SEC files grouped by ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2944b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary of SEC files grouped by ticker\n",
    "sec_files_by_ticker = {}\n",
    "\n",
    "# Files to exclude (not actual SEC filings)\n",
    "exclude_prefixes = ['cik', 'sec_files', 'sec_filings']\n",
    "\n",
    "for f in sec_files:\n",
    "    # Extract ticker (characters before first underscore)\n",
    "    if '_' in f:\n",
    "        ticker = f.split('_')[0]\n",
    "    else:\n",
    "        ticker = f  # Use full filename if no underscore\n",
    "    \n",
    "    # Skip non-filing files (cik_lookup.json, cik_lookup.csv, sec_files_by_ticker.json, etc.)\n",
    "    if ticker.lower() in exclude_prefixes:\n",
    "        continue\n",
    "    \n",
    "    if ticker not in sec_files_by_ticker:\n",
    "        sec_files_by_ticker[ticker] = []\n",
    "    sec_files_by_ticker[ticker].append(f)\n",
    "\n",
    "# Display results\n",
    "print(\"-\" * 60)\n",
    "print(f\"Unique tickers with SEC filings: {len(sec_files_by_ticker)}\")\n",
    "print(\"-\" * 60)\n",
    "for ticker, files in sorted(sec_files_by_ticker.items()):\n",
    "    print(f\"\\n{ticker} ({len(files)} files):\")\n",
    "    for file in files:\n",
    "        print(f\"  - {file}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Save the dictionary to a JSON file in the SEC filings directory\n",
    "sec_files_dict_path = os.path.join(OUTPUT_DIR_SEC_FILINGS, \"sec_files_by_ticker.json\")\n",
    "with open(sec_files_dict_path, 'w') as f:\n",
    "    json.dump(sec_files_by_ticker, f, indent=2)\n",
    "print(f\"\\n✅ SEC files dictionary saved to: {sec_files_dict_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe52119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR_INDIVIDUAL_STOCK_ANALYSIS, exist_ok=True)  \n",
    "\n",
    "# Initialize Perplexity client\n",
    "client = PerplexityClient(api_key=API_KEY)\n",
    "counter = 0\n",
    "\n",
    "# Helper function to read SEC filing content\n",
    "def read_sec_filing_content(filepath, max_chars=50000):\n",
    "    \"\"\"Read SEC filing HTML and extract text content, truncated to max_chars\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        # Simple HTML tag removal for text extraction\n",
    "        text = re.sub(r'<[^>]+>', ' ', content)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        if len(text) > max_chars:\n",
    "            text = text[:max_chars] + \"... [truncated]\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Could not read {filepath}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304435d",
   "metadata": {},
   "source": [
    "## Generate Analysis for Individual Equity and in the List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process each individual equity (not ETFs or mutual funds)\n",
    "for equity in individual_equities:\n",
    "    print(equity)\n",
    "    # Check if report already exists for today\n",
    "    date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "    output_filename = f\"Equity Report - {equity} {date_str}.docx\"\n",
    "    output_path = os.path.join(OUTPUT_DIR_INDIVIDUAL_STOCK_ANALYSIS, output_filename)\n",
    " \n",
    "    if os.path.exists(output_path):\n",
    "        #print(f\"⏭️  Skipping {equity} - report already exists for {date_str}\")\n",
    "        continue\n",
    "\n",
    "    # Gather SEC filing content for this ticker\n",
    "    sec_content = \"\"\n",
    "    sec_files_used = []\n",
    "    if equity in sec_files_by_ticker:\n",
    "        print(f\"  Loading {len(sec_files_by_ticker[equity])} SEC filings for {equity}...\")\n",
    "        for sec_file in sec_files_by_ticker[equity]:\n",
    "            filepath = os.path.join(OUTPUT_DIR_SEC_FILINGS, sec_file)\n",
    "            file_content = read_sec_filing_content(filepath, max_chars=30000)\n",
    "            if file_content:\n",
    "                sec_files_used.append(sec_file)\n",
    "                sec_content += f\"\\n\\n--- SEC Filing: {sec_file} ---\\n{file_content}\"\n",
    "    \n",
    "    # Construct prompt with equity ticker, SEC filings, and template\n",
    "    if sec_content:\n",
    "        prompt = f\"For the equity {equity}, analyze the following SEC filings and {PROMPT_TEMPLATE}\\n\\nSEC FILINGS:\\n{sec_content}\"\n",
    "    else:\n",
    "        prompt = f\"For the equity {equity} {PROMPT_TEMPLATE}\"\n",
    "\n",
    "    try:\n",
    "        # Query the LLM\n",
    "        print(f\"Querying Perplexity API for {equity} ({counter + 1}/{len(individual_equities)})...\")\n",
    "        if sec_files_used:\n",
    "            print(f\"  Including SEC filings: {', '.join(sec_files_used)}\")\n",
    "\n",
    "        generated_text = client.chat(prompt, model=MODEL)\n",
    "        counter += 1\n",
    "        \n",
    "        # Create Word document\n",
    "        doc = Document()\n",
    "        doc.add_heading(f\"Market Outlook Report - {equity}\", 0)\n",
    "        doc.add_paragraph(f\"Perplexity Sonar Pro Model Generated: {date_str}\")\n",
    "        doc.add_paragraph()\n",
    "        \n",
    "        # Add SEC filings used\n",
    "        if sec_files_used:\n",
    "            doc.add_heading(\"SEC Filings Analyzed:\", level=2)\n",
    "            for sf in sec_files_used:\n",
    "                doc.add_paragraph(sf, style='List Bullet')\n",
    "            doc.add_paragraph()\n",
    "        \n",
    "        # Convert markdown to Word formatting\n",
    "        add_markdown_to_word(doc, generated_text)\n",
    "        \n",
    "        # Add prompt info at the end (without the full SEC content for readability)\n",
    "        doc.add_paragraph()\n",
    "        doc.add_heading(\"Prompt Used:\", level=2)\n",
    "        prompt_summary = f\"For the equity {equity} {PROMPT_TEMPLATE}\"\n",
    "        if sec_files_used:\n",
    "            prompt_summary += f\"\\n\\n[SEC filings included: {', '.join(sec_files_used)}]\"\n",
    "        doc.add_paragraph(prompt_summary)\n",
    "        \n",
    "        # Save the document\n",
    "        doc.save(output_path)\n",
    "        # print(f\"✅ Saved: {output_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {equity}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ Completed processing {len(individual_equities)} individual equities\")\n",
    "print(f\"Reports saved to: {OUTPUT_DIR_INDIVIDUAL_STOCK_ANALYSIS}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535a2e0",
   "metadata": {},
   "source": [
    "## Generate Analysis for Each ETF and in the List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each ETF with SEC filing integration\n",
    "# Output directory for ETF reports (can be same as individual or separate)\n",
    "OUTPUT_DIR_ETF_ANALYSIS = os.getenv(\"Output_dir_etf_analysis\", OUTPUT_DIR_INDIVIDUAL_STOCK_ANALYSIS)\n",
    "os.makedirs(OUTPUT_DIR_ETF_ANALYSIS, exist_ok=True)\n",
    "\n",
    "etf_counter = 0\n",
    "\n",
    "for etf in etfs:\n",
    "    print(etf)\n",
    "    # Check if report already exists for today\n",
    "    date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "    output_filename = f\"ETF Report - {etf} {date_str}.docx\"\n",
    "    output_path = os.path.join(OUTPUT_DIR_ETF_ANALYSIS, output_filename)\n",
    " \n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"⏭️  Skipping {etf} - report already exists for {date_str}\")\n",
    "        continue\n",
    "\n",
    "    # Gather SEC filing content for this ETF\n",
    "    sec_content = \"\"\n",
    "    sec_files_used = []\n",
    "    if etf in sec_files_by_ticker:\n",
    "        print(f\"  Loading {len(sec_files_by_ticker[etf])} SEC filings for {etf}...\")\n",
    "        for sec_file in sec_files_by_ticker[etf]:\n",
    "            filepath = os.path.join(OUTPUT_DIR_SEC_FILINGS, sec_file)\n",
    "            file_content = read_sec_filing_content(filepath, max_chars=30000)\n",
    "            if file_content:\n",
    "                sec_files_used.append(sec_file)\n",
    "                sec_content += f\"\\n\\n--- SEC Filing: {sec_file} ---\\n{file_content}\"\n",
    "    \n",
    "    # Construct prompt with ETF ticker, SEC filings, and ETF-specific template\n",
    "    if sec_content:\n",
    "        prompt = f\"For the ETF {etf}, analyze the following SEC filings and {PROMPT_ETF_ANALYSIS_FILE_TEMPLATE}\\n\\nSEC FILINGS:\\n{sec_content}\"\n",
    "    else:\n",
    "        prompt = f\"For the ETF {etf} {PROMPT_ETF_ANALYSIS_FILE_TEMPLATE}\"\n",
    "\n",
    "    try:\n",
    "        # Query the LLM\n",
    "        print(f\"Querying Perplexity API for ETF {etf} ({etf_counter + 1}/{len(etfs)})...\")\n",
    "        if sec_files_used:\n",
    "            print(f\"  Including SEC filings: {', '.join(sec_files_used)}\")\n",
    "\n",
    "        generated_text = client.chat(prompt, model=MODEL)\n",
    "        etf_counter += 1\n",
    "        \n",
    "        # Create Word document\n",
    "        doc = Document()\n",
    "        doc.add_heading(f\"ETF Analysis Report - {etf}\", 0)\n",
    "        doc.add_paragraph(f\"Perplexity Sonar Pro Model Generated: {date_str}\")\n",
    "        doc.add_paragraph()\n",
    "        \n",
    "        # Add SEC filings used\n",
    "        if sec_files_used:\n",
    "            doc.add_heading(\"SEC Filings Analyzed:\", level=2)\n",
    "            for sf in sec_files_used:\n",
    "                doc.add_paragraph(sf, style='List Bullet')\n",
    "            doc.add_paragraph()\n",
    "        \n",
    "        # Convert markdown to Word formatting\n",
    "        add_markdown_to_word(doc, generated_text)\n",
    "        \n",
    "        # Add prompt info at the end (without the full SEC content for readability)\n",
    "        # doc.add_paragraph()\n",
    "        # doc.add_heading(\"Prompt Used:\", level=2)\n",
    "        # prompt_summary = f\"For the ETF {etf} {PROMPT_ETF_ANALYSIS_FILE_TEMPLATE}\"\n",
    "        # if sec_files_used:\n",
    "        #     prompt_summary += f\"\\n\\n[SEC filings included: {', '.join(sec_files_used)}]\"\n",
    "        # doc.add_paragraph(prompt_summary)\n",
    "        \n",
    "        # Save the document\n",
    "        doc.save(output_path)\n",
    "        print(f\"✅ Saved: {output_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing ETF {etf}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ Completed processing {len(etfs)} ETFs\")\n",
    "print(f\"Reports saved to: {OUTPUT_DIR_ETF_ANALYSIS}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cdeda8",
   "metadata": {},
   "source": [
    "## Generate Analysis for Each Mutual Fund in the List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a58956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each Mutual Fund with SEC filing integration\n",
    "# Output directory for Mutual Fund reports (can be same as individual or separate)\n",
    "OUTPUT_DIR_MUTUAL_FUND_ANALYSIS = os.getenv(\"Output_dir_mutual_fund_analysis\", OUTPUT_DIR_INDIVIDUAL_STOCK_ANALYSIS)\n",
    "os.makedirs(OUTPUT_DIR_MUTUAL_FUND_ANALYSIS, exist_ok=True)\n",
    "\n",
    "# Use the ETF analysis template for mutual funds (or define a separate one if needed)\n",
    "PROMPT_MUTUAL_FUND_TEMPLATE = PROMPT_ETF_ANALYSIS_FILE_TEMPLATE\n",
    "\n",
    "mf_counter = 0\n",
    "\n",
    "for mf in mutual_funds:\n",
    "    \n",
    "    # Check if report already exists for today\n",
    "    date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "    output_filename = f\"Mutual Fund Report - {mf} {date_str}.docx\"\n",
    "    output_path = os.path.join(OUTPUT_DIR_MUTUAL_FUND_ANALYSIS, output_filename)\n",
    " \n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"⏭️  Skipping {mf} - report already exists for {date_str}\")\n",
    "        continue\n",
    "\n",
    "    # Gather SEC filing content for this mutual fund\n",
    "    sec_content = \"\"\n",
    "    sec_files_used = []\n",
    "    if mf in sec_files_by_ticker:\n",
    "        print(f\"  Loading {len(sec_files_by_ticker[mf])} SEC filings for {mf}...\")\n",
    "        for sec_file in sec_files_by_ticker[mf]:\n",
    "            filepath = os.path.join(OUTPUT_DIR_SEC_FILINGS, sec_file)\n",
    "            file_content = read_sec_filing_content(filepath, max_chars=30000)\n",
    "            if file_content:\n",
    "                sec_files_used.append(sec_file)\n",
    "                sec_content += f\"\\n\\n--- SEC Filing: {sec_file} ---\\n{file_content}\"\n",
    "    \n",
    "    # Construct prompt with mutual fund ticker, SEC filings, and template\n",
    "    if sec_content:\n",
    "        prompt = f\"For the Mutual Fund {mf}, analyze the following SEC filings and {PROMPT_MUTUAL_FUND_TEMPLATE}\\n\\nSEC FILINGS:\\n{sec_content}\"\n",
    "    else:\n",
    "        prompt = f\"For the Mutual Fund {mf} {PROMPT_MUTUAL_FUND_TEMPLATE}\"\n",
    "\n",
    "    try:\n",
    "        # Query the LLM\n",
    "        print(f\"Querying Perplexity API for Mutual Fund {mf} ({mf_counter + 1}/{len(mutual_funds)})...\")\n",
    "        if sec_files_used:\n",
    "            print(f\"  Including SEC filings: {', '.join(sec_files_used)}\")\n",
    "\n",
    "        generated_text = client.chat(prompt, model=MODEL)\n",
    "        mf_counter += 1\n",
    "        \n",
    "        # Create Word document\n",
    "        doc = Document()\n",
    "        doc.add_heading(f\"Mutual Fund Analysis Report - {mf}\", 0)\n",
    "        doc.add_paragraph(f\"Perplexity Sonar Pro Model Generated: {date_str}\")\n",
    "        doc.add_paragraph()\n",
    "        \n",
    "        # Add SEC filings used\n",
    "        if sec_files_used:\n",
    "            doc.add_heading(\"SEC Filings Analyzed:\", level=2)\n",
    "            for sf in sec_files_used:\n",
    "                doc.add_paragraph(sf, style='List Bullet')\n",
    "            doc.add_paragraph()\n",
    "        \n",
    "        # Convert markdown to Word formatting\n",
    "        add_markdown_to_word(doc, generated_text)\n",
    "        \n",
    "        # Add prompt info at the end (without the full SEC content for readability)\n",
    "        doc.add_paragraph()\n",
    "        doc.add_heading(\"Prompt Used:\", level=2)\n",
    "        prompt_summary = f\"For the Mutual Fund {mf} {PROMPT_MUTUAL_FUND_TEMPLATE}\"\n",
    "        if sec_files_used:\n",
    "            prompt_summary += f\"\\n\\n[SEC filings included: {', '.join(sec_files_used)}]\"\n",
    "        doc.add_paragraph(prompt_summary)\n",
    "        \n",
    "        # Save the document\n",
    "        doc.save(output_path)\n",
    "        print(f\"✅ Saved: {output_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing Mutual Fund {mf}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ Completed processing {len(mutual_funds)} Mutual Funds\")\n",
    "print(f\"Reports saved to: {OUTPUT_DIR_MUTUAL_FUND_ANALYSIS}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce53cc",
   "metadata": {},
   "source": [
    "## Generate Ratings Change Report and Notifications for Individual Equities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d8e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Ratings Change Report for select equities\n",
    "from docx import Document\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR_PORTFOLIO_ANALYSIS, exist_ok=True)\n",
    "\n",
    "date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "output_filename = f\"Ratings Change Report {date_str}.docx\"\n",
    "output_path = os.path.join(OUTPUT_DIR_PORTFOLIO_ANALYSIS, output_filename)\n",
    "\n",
    "doc = Document()\n",
    "doc.add_heading(\"Ratings Change Report\", 0)\n",
    "doc.add_paragraph(f\"Perplexity Sonar Pro Model Generated: {date_str}\")\n",
    "doc.add_paragraph()\n",
    "\n",
    "#Summary table API call time: {t1-t0:.2f} seconds\n",
    "# --- Individual equity details ---\n",
    "\n",
    "for equity in individual_equities:\n",
    "    prompt = f\"For the equity {equity} {PROMPT_RATINGS_CHANGE_TEMPLATE}\"\n",
    "    doc.add_heading(f\"{equity}\", level=1)\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        generated_text = client.chat(prompt, model=MODEL)\n",
    "        t1 = time.time()\n",
    "        add_markdown_to_word(doc, generated_text)\n",
    "        # print(f\"{equity} API call time: {t1-t0:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        doc.add_paragraph(f\"❌ Error processing {equity}: {e}\")\n",
    "    doc.add_paragraph()  # Space between equities\n",
    "\n",
    "# Add prompt template at the end for reference\n",
    "doc.add_heading(\"Prompt Template Used:\", level=2)\n",
    "doc.add_paragraph(PROMPT_RATINGS_CHANGE_TEMPLATE)\n",
    "\n",
    "doc.save(output_path)\n",
    "print(f\"✅ Saved: {output_filename} in {OUTPUT_DIR_PORTFOLIO_ANALYSIS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2dbe42",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
